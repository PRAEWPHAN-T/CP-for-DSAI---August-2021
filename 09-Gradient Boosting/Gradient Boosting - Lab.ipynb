{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Modify the Gradient Boosting scratch code in our lecture such that:\r\n",
    "- Notice that we are still using max_depth = 1.  Attempt to tweak min_samples_split, max_depth for the regression and see whether we can achieve better mse on our boston data\r\n",
    "- Notice that we only write scratch code for gradient boosting for regression, add some code so that it also works for binary classification.  Load the breast cancer data from sklearn and see that it works.\r\n",
    "- Further change the code so that it works for multiclass classification.  Load the digits data from sklearn and see that it works\r\n",
    "- Put everything into class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from scipy.special import expit\r\n",
    "from sklearn.tree import DecisionTreeRegressor\r\n",
    "from sklearn.dummy import DummyRegressor\r\n",
    "from sklearn.ensemble import GradientBoostingRegressor\r\n",
    "from sklearn.dummy import DummyClassifier\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.datasets import load_digits\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "class GradientBoosting:\r\n",
    "    def __init__(self, S=5, learning_rate=1, max_depth = 1, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=True, tol=1e-4):\r\n",
    "        self.S = S\r\n",
    "        self.learning_rate = learning_rate\r\n",
    "        self.max_depth = max_depth\r\n",
    "        self.min_samples_split = min_samples_split\r\n",
    "        self.regression=regression\r\n",
    "\r\n",
    "        #initialize regression trees\r\n",
    "        tree_params = {'max_depth': self.max_depth,\r\n",
    "                      'min_samples_split': self.min_samples_split}\r\n",
    "        self.models = [DecisionTreeRegressor(**tree_params) for _ in range(S)]         #create tree\r\n",
    "        first_model = DummyRegressor(strategy='mean')  #use mean / stump\r\n",
    "        if regression == True:\r\n",
    "            # if GBregressor use Regressor as the first model\r\n",
    "            first_model = DummyRegressor(strategy='mean')\r\n",
    "        else :\r\n",
    "            # if GBclassifier use Classifier as the first model\r\n",
    "            first_model = DummyClassifier(strategy='most_frequent')\r\n",
    "            \r\n",
    "        self.models.insert(0, first_model) #add mean y to models\r\n",
    "\r\n",
    "\r\n",
    "    def grad(self, y, h):  #div loss\r\n",
    "        return y - h \r\n",
    "\r\n",
    "    def predict(self, X, models=None, with_argmax=True):\r\n",
    "        if models is None:\r\n",
    "            models = self.models\r\n",
    "        h0 = models[0].predict(X)  #first use the dummy model\r\n",
    "        boosting = sum(self.learning_rate * model.predict(X) for model in models[1:])\r\n",
    "        yhat = h0 + boosting\r\n",
    "        \r\n",
    "        if not self.regression:\r\n",
    "            #turn into probability using softmax\r\n",
    "            yhat = np.exp(yhat) / np.sum(np.exp(yhat), axis=1, keepdims=True)\r\n",
    "            if with_argmax:\r\n",
    "                yhat = np.argmax(yhat, axis=1)\r\n",
    "        return yhat\r\n",
    "    \r\n",
    "    def fit(self, X, y):  #<----X_train\r\n",
    "        \r\n",
    "        #fit the first model\r\n",
    "        self.models[0].fit(X, y)\r\n",
    "        \r\n",
    "        for i in range(self.S):\r\n",
    "            #predict\r\n",
    "            yhat = self.predict(X, self.models[:i+1], with_argmax=False)\r\n",
    "            \r\n",
    "            #get the gradient\r\n",
    "            gradient = self.grad(y, yhat)\r\n",
    "            \r\n",
    "            #fit the next model with gradient\r\n",
    "            self.models[i+1].fit(X, gradient)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Regression\r\n",
    "from sklearn.datasets import load_boston\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "X, y = load_boston(return_X_y=True)\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \r\n",
    "                        test_size=0.3, random_state=42)\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 1, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=True, tol=1e-4)                     # set regresssion here\r\n",
    "model.fit(X_train, y_train)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "print(X_test[1])\r\n",
    "print(yhat[1])\r\n",
    "#print metrics\r\n",
    "print(\"MSE max_depth =1: \", mean_squared_error(y_test, yhat))\r\n",
    "\r\n",
    "#=====SKlearn========\r\n",
    "#Compare to sklearn: ls is the same as our mse\r\n",
    "n_estimators = 200\r\n",
    "sklearn_model = GradientBoostingRegressor(\r\n",
    "    n_estimators=n_estimators,\r\n",
    "    learning_rate = 0.1,\r\n",
    "    max_depth=1,\r\n",
    "    loss='ls'\r\n",
    ")\r\n",
    "\r\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\r\n",
    "print(\"Sklearn MSE max_depth= 1: \", mean_squared_error(y_test, yhat_sk))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[5.6440e-02 4.0000e+01 6.4100e+00 1.0000e+00 4.4700e-01 6.7580e+00\n",
      " 3.2900e+01 4.0776e+00 4.0000e+00 2.5400e+02 1.7600e+01 3.9690e+02\n",
      " 3.5300e+00]\n",
      "33.04343085396104\n",
      "MSE max_depth =1:  12.945557601580582\n",
      "Sklearn MSE max_depth= 1:  12.945557601580587\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Attempt to tweak min_samples_split, max_depth for the regression and see whether we can achieve better mse on our boston data\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 1, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=True, tol=1e-4)                     # set regresssion here\r\n",
    "model.fit(X_train, y_train)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "#print metrics\r\n",
    "print(\"MSE max_depth =1,min_sample= 2: \", mean_squared_error(y_test, yhat))\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 3, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=True, tol=1e-4)                     # set regresssion here\r\n",
    "model.fit(X_train, y_train)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "#print metrics\r\n",
    "print(\"MSE max_depth =3,min_sample= 2: \", mean_squared_error(y_test, yhat))\r\n",
    "\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 5, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=True, tol=1e-4)                     # set regresssion here\r\n",
    "model.fit(X_train, y_train)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "#print metrics\r\n",
    "print(\"MSE max_depth =5,min_sample= 2: \", mean_squared_error(y_test, yhat))\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 7, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=True, tol=1e-4)                     # set regresssion here\r\n",
    "model.fit(X_train, y_train)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "#print metrics\r\n",
    "print(\"MSE max_depth =7,min_sample= 2: \", mean_squared_error(y_test, yhat))\r\n",
    "\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 1, \r\n",
    "                 min_samples_split = 4,\r\n",
    "                 regression=True, tol=1e-4)                     # set regresssion here\r\n",
    "model.fit(X_train, y_train)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "#print metrics\r\n",
    "print(\"MSE max_depth =1, min_sample= 4 : \", mean_squared_error(y_test, yhat))\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 7, \r\n",
    "                 min_samples_split = 14,\r\n",
    "                 regression=True, tol=1e-4)                     # set regresssion here\r\n",
    "model.fit(X_train, y_train)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "#print metrics\r\n",
    "print(\"MSE max_depth =7, min_sample= 14 : \", mean_squared_error(y_test, yhat))\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 7, \r\n",
    "                 min_samples_split = 16,\r\n",
    "                 regression=True, tol=1e-4)                     # set regresssion here\r\n",
    "model.fit(X_train, y_train)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "#print metrics\r\n",
    "print(\"MSE max_depth =3, min_sample= 16 : \", mean_squared_error(y_test, yhat))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "MSE max_depth =1,min_sample= 2:  12.945557601580582\n",
      "MSE max_depth =3,min_sample= 2:  8.15206238292054\n",
      "MSE max_depth =5,min_sample= 2:  7.936777040494664\n",
      "MSE max_depth =7,min_sample= 2:  7.321226464228971\n",
      "MSE max_depth =1, min_sample= 4 :  12.945557601580582\n",
      "MSE max_depth =7, min_sample= 14 :  6.995328028543036\n",
      "MSE max_depth =3, min_sample= 16 :  8.39413341725854\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change max_depth and min_sample at proper number can achieve better mse on our boston data.\r\n",
    "\r\n",
    "In this experiment max_depth =7, min_sample= 14 are the best number that make mse is the least value.\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Binary classification\r\n",
    "from sklearn.datasets import load_breast_cancer\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.ensemble import GradientBoostingClassifier\r\n",
    "\r\n",
    "X, y = load_breast_cancer(return_X_y=True)\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = \\\r\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\r\n",
    "y_train_encoded = np.zeros((y_train.shape[0], len(set(y))))\r\n",
    "for each_class in range(len(set(y))):\r\n",
    "    cond = y_train==each_class\r\n",
    "    y_train_encoded[np.where(cond), each_class] = 1\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 3, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=False)    # set regresssion here\r\n",
    "model.fit(X_train, y_train_encoded)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "print(X_test[1])\r\n",
    "print(yhat[1])\r\n",
    "# #print metrics\r\n",
    "print(\"Our accuracy: \", accuracy_score(y_test, yhat))\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "#=====SKlearn========\r\n",
    "#Compare to sklearn: ls is the same as our accuracy\r\n",
    "n_estimators = 200\r\n",
    "sklearn_model = GradientBoostingClassifier(\r\n",
    "    n_estimators=n_estimators,\r\n",
    "    learning_rate = 0.1,\r\n",
    "    max_depth=1\r\n",
    ")\r\n",
    "\r\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\r\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, yhat_sk))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1.894e+01 2.131e+01 1.236e+02 1.130e+03 9.009e-02 1.029e-01 1.080e-01\n",
      " 7.951e-02 1.582e-01 5.461e-02 7.888e-01 7.975e-01 5.486e+00 9.605e+01\n",
      " 4.444e-03 1.652e-02 2.269e-02 1.370e-02 1.386e-02 1.698e-03 2.486e+01\n",
      " 2.658e+01 1.659e+02 1.866e+03 1.193e-01 2.336e-01 2.687e-01 1.789e-01\n",
      " 2.551e-01 6.589e-02]\n",
      "0\n",
      "Our accuracy:  0.9707602339181286\n",
      "Sklearn accuracy:  0.9649122807017544\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "# Multiclass classification\r\n",
    "# from sklearn.datasets import load_iris\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.ensemble import GradientBoostingClassifier\r\n",
    "\r\n",
    "X, y = load_digits(return_X_y=True)\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = \\\r\n",
    "        train_test_split(X, y, test_size=0.3, random_state=42)\r\n",
    "y_train_encoded = np.zeros((y_train.shape[0], len(set(y))))\r\n",
    "\r\n",
    "#encode method\r\n",
    "for each_class in range(len(set(y))):\r\n",
    "    cond = y_train==each_class\r\n",
    "    y_train_encoded[np.where(cond), each_class] = 1\r\n",
    "\r\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 1, \r\n",
    "                 min_samples_split = 2,\r\n",
    "                 regression=False)  # set regresssion here\r\n",
    "model.fit(X_train, y_train_encoded)\r\n",
    "yhat = model.predict(X_test)\r\n",
    "print(X_test[1])\r\n",
    "print(yhat[1])\r\n",
    "# #print metrics\r\n",
    "print(\"Our accuracy: \", accuracy_score(y_test, yhat))\r\n",
    "\r\n",
    "#=====SKlearn========\r\n",
    "#Compare to sklearn: ls is the same as our accuracy\r\n",
    "n_estimators = 200\r\n",
    "sklearn_model = GradientBoostingClassifier(\r\n",
    "    n_estimators=n_estimators,\r\n",
    "    learning_rate = 0.1,\r\n",
    "    max_depth=1\r\n",
    ")\r\n",
    "\r\n",
    "yhat_sk = sklearn_model.fit(X_train, y_train).predict(X_test)\r\n",
    "print(\"Sklearn accuracy: \", accuracy_score(y_test, yhat_sk))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[ 0.  0. 11. 16.  8.  0.  0.  0.  0.  6. 16. 11. 13.  9.  0.  0.  0.  7.\n",
      " 16.  0.  9. 16.  0.  0.  0.  2. 15. 12. 16. 16.  3.  0.  0.  0.  5.  7.\n",
      "  7. 16.  4.  0.  0.  0.  0.  0.  5. 16.  5.  0.  0.  0.  3.  7. 16. 11.\n",
      "  0.  0.  0.  0. 13. 16. 11.  1.  0.  0.]\n",
      "9\n",
      "Our accuracy:  0.8055555555555556\n",
      "Sklearn accuracy:  0.9481481481481482\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "interpreter": {
   "hash": "9d3bae0a0f66551680ef8a166f6b92cc2774d5d7901f027deb7bb883ed06d5ae"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}