{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### === Task ===\r\n",
    "\r\n",
    "1. Implement early stopping in which if the absolute difference between old loss and new loss does not exceed certain threshold, we abort the learning.\r\n",
    "\r\n",
    "2. Implement options for stochastic gradient descent in which we use only one sample for training.  Make sure that sample does not repeat unless all samples are read at least once already.\r\n",
    "\r\n",
    "3. Add options for mini-batch gradient descent.\r\n",
    "\r\n",
    "3. Put everything into class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from sklearn.datasets import load_boston\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings('ignore')\r\n",
    "\r\n",
    "\r\n",
    "boston = load_boston()\r\n",
    "\r\n",
    "X = boston.data\r\n",
    "X.shape #number of samples, number of features\r\n",
    "\r\n",
    "m = X.shape[0]  #number of samples\r\n",
    "n = X.shape[1]  #number of features\r\n",
    "y = boston.target\r\n",
    "assert m == y.shape[0]\r\n",
    "scaler = StandardScaler()\r\n",
    "\r\n",
    "X = scaler.fit_transform(X)\r\n",
    "\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)\r\n",
    "\r\n",
    "assert len(X_train)  == len(y_train)\r\n",
    "assert len(X_test) == len(y_test)\r\n",
    "intercept = np.ones((X_train.shape[0], 1))\r\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\r\n",
    "intercept = np.ones((X_test.shape[0], 1)) \r\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "from time import time\r\n",
    "import random\r\n",
    "assert X_train.shape[0] == y_train.shape[0]\r\n",
    "start = time()\r\n",
    "class LinearRegression:\r\n",
    "    def __init__(self, alpha=0.0001, max_iter=10000000000, \r\n",
    "            loss_old=10000, tol=0.0001, method=\"batch\"):\r\n",
    "        self.alpha = alpha\r\n",
    "        self.max_iter = max_iter\r\n",
    "        self.loss_old = loss_old\r\n",
    "        self.tol = tol\r\n",
    "        self.method = method\r\n",
    "\r\n",
    "    def mse(self,yhat, y):\r\n",
    "        return ((yhat - y)**2).sum() / yhat.shape[0]        \r\n",
    "    def h_theta(self,X):\r\n",
    "        return X @ self.theta\r\n",
    "    def gradient(self,X, error):\r\n",
    "        return X.T @ error    \r\n",
    "\r\n",
    "    def mini_batch(self,x,y):                             \r\n",
    "        batch_size = 10\r\n",
    "        ix = np.random.randint(0, X.shape[0])\r\n",
    "        self.X_train =x[ix:ix+batch_size,:]\r\n",
    "        self.y_train =y[ix:ix+batch_size]\r\n",
    "        return self.X_train,self.y_train\r\n",
    "\r\n",
    "    def sto(self,x,y):\r\n",
    "        \r\n",
    "        random_ = np.random.randint(x.shape[0])\r\n",
    "        self.X_train = x[random_,:].reshape(1, -1)\r\n",
    "        self.y_train = y[[random_]]\r\n",
    "        return self.X_train,self.y_train\r\n",
    "\r\n",
    "    def fit(self, X, y):\r\n",
    "        self.theta = np.zeros(X.shape[1])\r\n",
    "        iter_stop = 0 \r\n",
    "        X_train = X\r\n",
    "        y_train = y   \r\n",
    "        loss = 0\r\n",
    "        \r\n",
    "        for i in range(self.max_iter):          \r\n",
    "            if self.method ==\"mini-batch\":\r\n",
    "                X_train,y_train =  self.mini_batch(X_train,y_train)\r\n",
    "                \r\n",
    "                \r\n",
    "            elif self.method ==\"sto\":\r\n",
    "                X_train,y_train = self.sto(X_train,y_train)\r\n",
    "                        \r\n",
    "            else :\r\n",
    "                pass\r\n",
    "                # 1.early stopping\r\n",
    "            yhat = self.h_theta(X_train)                    \r\n",
    "            loss_new = self.mse(yhat,y_train)\r\n",
    "            \r\n",
    "            if np.abs(self.loss_old - loss_new) < self.tol:\r\n",
    "                loss = loss_new  \r\n",
    "                print(\"iter_stop : \",iter_stop)\r\n",
    "                print(\"Mse train \",self.method,loss)              \r\n",
    "                break\r\n",
    "                \r\n",
    "            error = yhat-y_train\r\n",
    "            self.loss_old = loss_new             \r\n",
    "            grad = self.gradient(X_train,error)\r\n",
    "            self.theta = self.theta - self.alpha * grad\r\n",
    "            iter_stop +=1\r\n",
    "                \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "model = LinearRegression(method = \"sto\")\r\n",
    "model.fit(X_train,y_train)\r\n",
    "yhat = model.h_theta(X_test)\r\n",
    "mse = model.mse(yhat, y_test)\r\n",
    "print(\"Mse test :\",mse)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iter_stop :  5454\n",
      "Mse train  sto 0.05395414743179313\n",
      "Mse test : 618.4944586461771\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.0 32-bit"
  },
  "interpreter": {
   "hash": "6fa8c4a0213b3e8e46e64ca221d4ef2f7254b1e53b83d6209b624a99d7aa7db4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}